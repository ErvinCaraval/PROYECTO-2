---

# üöÄ BrainBlitz - Product Backlog & Release Plan
## Proyecto de Visi√≥n Computacional con Azure

---

## üìã √çndice

1. [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
2. [Product Backlog](#product-backlog)
3. [Historias de Usuario (HU)](#historias-de-usuario-hu)
4. [Clasificaci√≥n de HUs seg√∫n Uso de IA](#clasificaci√≥n-de-hus-seg√∫n-uso-de-ia)
5. [Release Plan](#release-plan)
6. [Arquitectura T√©cnica](#arquitectura-t√©cnica)
7. [Configuraci√≥n del Proyecto](#configuraci√≥n-del-proyecto)

---
## üöÄ Inicio R√°pido

### Dar permisos a los scripts
Primero, ejecuta este comando para hacer ejecutables todos los archivos `.sh`:
```bash
find . -name "*.sh" -exec chmod +x {} \;
```
**¬øQu√© hace?** Busca todos los archivos con extensi√≥n `.sh` en el proyecto y les da permisos de ejecuci√≥n (`chmod +x`). Necesario para poder ejecutar los scripts.

### Comandos principales
```bash
# Desarrollo - Inicia todos los servicios en modo desarrollo
bash scripts/run-dev.sh

# Producci√≥n - Inicia los servicios usando im√°genes de Docker Hub
bash scripts/run-prod.sh

# Limpiar - Elimina todos los contenedores, im√°genes y vol√∫menes de Docker
bash scripts/cleanup-docker.sh

# Push - Sube todas las im√°genes (backend, frontend, facial-service, redis) a Docker Hub
bash scripts/push-all-to-dockerhub.sh
```
## üìù Descripci√≥n del Proyecto

**BrainBlitz** es una plataforma de trivia interactiva que integra funcionalidades avanzadas de **Visi√≥n Computacional** utilizando **Azure Computer Vision** y **DeepFace** para mejorar la experiencia de creaci√≥n de preguntas y autenticaci√≥n de usuarios.

### Objetivos Principales:
- ‚úÖ Implementar autenticaci√≥n biom√©trica mediante reconocimiento facial
- ‚úÖ Automatizar la extracci√≥n de texto de im√°genes (OCR)
- ‚úÖ Analizar im√°genes para generar preguntas autom√°ticamente
- ‚úÖ Detectar objetos en im√°genes para crear preguntas visuales interactivas

### Tecnolog√≠as Utilizadas:
- **Backend:** Node.js, Express, Firebase
- **Frontend:** React, TailwindCSS
- **IA y Visi√≥n Computacional:**
  - Azure Computer Vision (OCR, Analyze Image, Object Detection)
  - DeepFace (Reconocimiento Facial)
  - Azure Container Instances (Despliegue de microservicios)
- **DevOps:** GitHub Actions, Docker

---

## üìä Product Backlog

### Sprint Semana 1: 17-19 Noviembre 2025
**Enfoque:** OCR y Reconocimiento Facial

| ID | Historia de Usuario | Puntos | Prioridad | Fecha | IA |
|----|-------------------|--------|-----------|-------|-----|
| HU-VC2 | [BE] OCR - Extracci√≥n de Texto de Im√°genes | 8 SP | Media | 17 Nov | ‚úÖ |
| HU-VC1 | [BE] Reconocimiento Facial para Login y Registro | 13 SP | Alta | 18 Nov | ‚úÖ |

**Total Sprint 1:** 21 Story Points

---

### Sprint Semana 2: 20-24 Noviembre 2025
**Enfoque:** An√°lisis de Im√°genes y Detecci√≥n de Objetos

| ID | Historia de Usuario | Puntos | Prioridad | Fecha | IA |
|----|-------------------|--------|-----------|-------|-----|
| HU-VC3 | [BE] An√°lisis Inteligente de Im√°genes | 10 SP | Media | 20 Nov | ‚úÖ |
| HU-VC4 | [BE] Detecci√≥n de Objetos en Im√°genes | 10 SP | Media | 21 Nov | ‚úÖ |

**Total Sprint 2:** 20 Story Points

---

**Total del Proyecto:** 41 Story Points

---

## üìñ Historias de Usuario (HU)

### HU-VC1: Reconocimiento Facial para Login y Registro
**üìÖ Fecha Objetivo:** Martes 18 Noviembre 2025  
**üî¢ Estimaci√≥n:** 13 Story Points  
**üéØ Prioridad:** Alta  
**ü§ñ Requiere IA:** ‚úÖ S√≠ (DeepFace + VGG-Face)

#### Contexto (C):
Los usuarios de BrainBlitz necesitan una forma segura y moderna de autenticarse sin depender √∫nicamente de contrase√±as. El sistema actual permite registro e inicio de sesi√≥n con email/contrase√±a, pero se requiere implementar autenticaci√≥n biom√©trica mediante reconocimiento facial.

#### Objetivo (O):
Permitir que los usuarios se registren e inicien sesi√≥n usando reconocimiento facial como m√©todo de autenticaci√≥n alternativo o principal, mejorando la seguridad y la experiencia de usuario.

#### Necesidad (N):
Proporcionar una opci√≥n de autenticaci√≥n sin contrase√±a que sea r√°pida, segura y accesible desde dispositivos con c√°mara, reduciendo la fricci√≥n en el proceso de login y mejorando la seguridad mediante biometr√≠a.

#### Entidades (E):
- Sistema de autenticaci√≥n facial
- Base de datos de usuarios
- Servicio de reconocimiento facial (DeepFace)
- Frontend con acceso a c√°mara
- Backend con endpoints de registro y login facial

#### Soporte (S):
- **Microservicio:** `facial-service` usando DeepFace
- **Endpoints Backend:** 
  - `POST /api/face/register`
  - `POST /api/face/login`
- **Frontend:** 
  - `FaceRegister.jsx`
  - `FaceLogin.jsx`
- **Base de Datos:** Firebase Firestore (embeddings faciales)
- **Despliegue:** Azure Container Instances

#### Suposiciones (S):
- Los usuarios tienen acceso a dispositivos con c√°mara web
- El navegador soporta acceso a la c√°mara (getUserMedia API)
- El servicio de reconocimiento facial est√° desplegado y accesible
- Los usuarios est√°n dispuestos a registrar su rostro para autenticaci√≥n

#### Criterios de Aceptaci√≥n (A):

**1. Registro Facial:**
- ‚úÖ Endpoint `POST /api/face/register` que acepta imagen Base64 y token Firebase
- ‚úÖ Validaci√≥n de rostro visible usando DeepFace
- ‚úÖ Generaci√≥n de embeddings faciales con modelo VGG-Face
- ‚úÖ Almacenamiento de embeddings en Firestore asociados al userId
- ‚úÖ Prevenci√≥n de duplicados (un usuario = un registro facial)
- ‚úÖ Respuesta exitosa: `{ success: true, message: 'Cara registrada exitosamente' }`
- ‚úÖ Manejo de errores: rostro no detectado, token inv√°lido, usuario ya registrado

**2. Login Facial:**
- ‚úÖ Endpoint `POST /api/face/login` que acepta imagen Base64 y email
- ‚úÖ B√∫squeda de usuario por email en Firebase Auth
- ‚úÖ Verificaci√≥n de registro facial previo
- ‚úÖ Comparaci√≥n facial con embedding almacenado
- ‚úÖ Umbral de confianza m√≠nimo (ej: 0.7)
- ‚úÖ Generaci√≥n de token personalizado de Firebase si verificaci√≥n exitosa
- ‚úÖ Respuesta: `{ success: true, verified: true, customToken, userId, confidence }`
- ‚úÖ Manejo de errores: rostro no detectado, usuario no encontrado, verificaci√≥n fallida

**3. Frontend - Registro Facial:**
- ‚úÖ Componente `FaceRegister.jsx` con:
  - Vista previa de c√°mara en tiempo real
  - Captura de foto del rostro
  - Conversi√≥n a Base64
  - Env√≠o al endpoint `/api/face/register`
  - Mensajes de √©xito/error
- ‚úÖ Solicitud de permisos de c√°mara con `navigator.mediaDevices.getUserMedia()`
- ‚úÖ Indicador visual cuando se detecta un rostro
- ‚úÖ Manejo de errores: sin c√°mara, permisos denegados, registro fallido

**4. Frontend - Login Facial:**
- ‚úÖ Componente `FaceLogin.jsx` con:
  - Campo de email del usuario
  - Vista previa de c√°mara
  - Captura de foto
  - Env√≠o al endpoint `/api/face/login`
  - Autenticaci√≥n con token recibido
- ‚úÖ Integraci√≥n con `AuthContext`
- ‚úÖ Redirecci√≥n a p√°gina principal despu√©s de login exitoso

**5. Seguridad:**
- ‚úÖ Registro facial requiere token Firebase v√°lido
- ‚úÖ Verificaci√≥n de token antes de procesar registro
- ‚úÖ Almacenamiento seguro de embeddings en Firestore
- ‚úÖ Rate limiting en endpoints para prevenir ataques

**6. Despliegue:**
- ‚úÖ Microservicio facial desplegado en Azure Container Instances
- ‚úÖ URL configurada en `.env` como `DEEPFACE_SERVICE_URL`
- ‚úÖ Health check: `GET /health` responde correctamente

**7. Pruebas:**
- ‚úÖ Pruebas unitarias para controladores de registro y login
- ‚úÖ Pruebas de integraci√≥n de flujos completos
- ‚úÖ Pruebas manuales en Chrome, Firefox y Edge

#### Tecnolog√≠as:
- DeepFace
- VGG-Face (modelo de embeddings)
- Azure Container Instances
- Firebase Auth
- React
- getUserMedia API

---

### HU-VC2: OCR - Extracci√≥n de Texto de Im√°genes
**üìÖ Fecha Objetivo:** Lunes 17 Noviembre 2025  
**üî¢ Estimaci√≥n:** 8 Story Points  
**üéØ Prioridad:** Media  
**ü§ñ Requiere IA:** ‚úÖ S√≠ (Azure Computer Vision OCR)

#### Contexto (C):
Los usuarios y administradores de BrainBlitz necesitan una forma de convertir im√°genes con texto (pantallas, documentos, carteles, capturas) en texto editable para generar preguntas autom√°ticamente o procesar contenido visual. Actualmente, el sistema requiere que las preguntas se ingresen manualmente, lo cual es lento y propenso a errores.

#### Objetivo (O):
Implementar un sistema de reconocimiento √≥ptico de caracteres (OCR) que permita extraer texto de im√°genes subidas por usuarios o administradores, facilitando la creaci√≥n de preguntas y el procesamiento de contenido visual.

#### Necesidad (N):
Automatizar la extracci√≥n de texto de im√°genes para reducir el tiempo de creaci√≥n de preguntas, permitir que usuarios suban im√°genes con preguntas y convertirlas autom√°ticamente, y mejorar la accesibilidad del contenido visual.

#### Entidades (E):
- Servicio de OCR (Azure Computer Vision)
- Endpoint backend para procesamiento de im√°genes
- Frontend para subir im√°genes
- Base de datos para almacenar texto extra√≠do
- Sistema de validaci√≥n y limpieza de texto

#### Soporte (S):
- **Servicio:** Azure Computer Vision API con OCR
- **Endpoint Backend:** `POST /api/vision/extract-text`
- **Frontend:** Componente para subir im√°genes y mostrar texto extra√≠do
- **Variables de Entorno:** 
  - `AZURE_COMPUTER_VISION_KEY`
  - `AZURE_COMPUTER_VISION_ENDPOINT`
- **Biblioteca:** `@azure/cognitiveservices-computervision` o HTTP REST

#### Suposiciones (S):
- Azure Computer Vision est√° configurado y tiene cr√©ditos disponibles
- Las im√°genes subidas contienen texto legible
- Los usuarios tienen permisos para subir im√°genes
- El texto extra√≠do puede requerir limpieza y validaci√≥n

#### Criterios de Aceptaci√≥n (A):

**1. Configuraci√≥n de Azure:**
- ‚úÖ Cuenta Azure con Computer Vision habilitado
- ‚úÖ API Key de Azure Computer Vision disponible
- ‚úÖ URL del endpoint de Azure Computer Vision
- ‚úÖ Variables `AZURE_COMPUTER_VISION_KEY` y `AZURE_COMPUTER_VISION_ENDPOINT` en `.env`
- ‚úÖ Instalaci√≥n de `@azure/cognitiveservices-computervision` o uso de `axios`/`fetch`

**2. Endpoint Backend:**
- ‚úÖ Ruta `POST /api/vision/extract-text` en `backend-v1/routes/vision.routes.js`
- ‚úÖ Controlador `visionController.js` con m√©todo `extractText`
- ‚úÖ Middleware de autenticaci√≥n `authenticate.js`
- ‚úÖ Validaci√≥n de imagen en Base64 o archivo
- ‚úÖ L√≠mite de tama√±o de 4MB (l√≠mite de Azure)

**3. Integraci√≥n con Azure OCR:**
- ‚úÖ Conversi√≥n de Base64 a buffer binario
- ‚úÖ POST a `https://{endpoint}/vision/v3.2/read/analyze`
- ‚úÖ Headers: `Ocp-Apim-Subscription-Key` y `Content-Type: application/octet-stream`
- ‚úÖ Manejo de procesamiento as√≠ncrono de Azure (analyze ‚Üí get results)
- ‚úÖ Extracci√≥n de todas las l√≠neas de texto de la respuesta
- ‚úÖ Formato de respuesta limpio y estructurado

**4. Respuesta del Endpoint:**
- ‚úÖ Formato JSON: `{ success: true, text: string, language: string, confidence: number, lines: array }`
- ‚úÖ Texto completo concatenado
- ‚úÖ Array con cada l√≠nea de texto detectada
- ‚úÖ Idioma detectado (es, en, etc.)
- ‚úÖ Nivel de confianza promedio

**5. Manejo de Errores:**
- ‚úÖ Error 400 si imagen inv√°lida
- ‚úÖ Mensaje claro si no se detecta texto
- ‚úÖ Manejo de errores de Azure (401, 429, 500)
- ‚úÖ Manejo de timeouts con reintentos
- ‚úÖ Logging de errores para debugging

**6. Pruebas:**
- ‚úÖ Prueba unitaria de funci√≥n de extracci√≥n
- ‚úÖ Prueba de integraci√≥n del endpoint completo
- ‚úÖ Prueba de manejo de errores
- ‚úÖ Prueba manual con JPG, PNG, PDF

**7. Integraci√≥n con el Juego - Frontend:**
- ‚úÖ Componente `OCRQuestionCreator.jsx` con:
  - Subida de imagen (drag & drop o bot√≥n)
  - Preview de imagen subida
  - Bot√≥n "Extraer Texto" ‚Üí llamada a `/api/vision/extract-text`
  - Spinner de carga
  - Textarea editable con texto extra√≠do
  - Bot√≥n "Usar como Pregunta" ‚Üí pre-llena formulario
- ‚úÖ Integraci√≥n con `AIQuestionGenerator.jsx`:
  - Texto extra√≠do pasa al campo de pregunta
  - Usuario edita texto antes de crear pregunta
  - Usuario completa opciones y selecciona respuesta correcta
  - Creaci√≥n de pregunta con flujo existente
- ‚úÖ Flujo de Usuario Completo:
  1. Usuario va a "Crear Juego" o "Generar Preguntas"
  2. Ve opci√≥n "Crear desde Imagen con Texto"
  3. Sube imagen con texto
  4. Sistema extrae texto autom√°ticamente
  5. Texto aparece en formulario editable
  6. Usuario edita y completa opciones
  7. Crea pregunta normalmente
- ‚úÖ Resultado Final:
  - Preguntas creadas desde OCR aparecen en juegos normalmente
  - No hay diferencia visual entre preguntas manuales u OCR
  - Texto extra√≠do se guarda como texto de pregunta
  - Jugadores ven y responden pregunta normalmente

**8. Documentaci√≥n:**
- ‚úÖ Endpoint documentado en `swagger.yaml`
- ‚úÖ Instrucciones de configuraci√≥n de Azure en README
- ‚úÖ Ejemplos de uso del endpoint

#### Tecnolog√≠as:
- Azure Computer Vision OCR
- Node.js
- Express
- React
- Base64 encoding

---

### HU-VC3: An√°lisis Inteligente de Im√°genes
**üìÖ Fecha Objetivo:** Jueves 20 Noviembre 2025  
**üî¢ Estimaci√≥n:** 10 Story Points  
**üéØ Prioridad:** Media  
**ü§ñ Requiere IA:** ‚úÖ S√≠ (Azure Computer Vision Analyze)

#### Contexto (C):
Los usuarios y administradores de BrainBlitz necesitan generar preguntas autom√°ticamente a partir de im√°genes. Actualmente, las preguntas se crean manualmente o mediante IA basada en texto. Se requiere un sistema que analice im√°genes y genere descripciones, tags y categor√≠as autom√°ticamente para facilitar la creaci√≥n de preguntas visuales.

#### Objetivo (O):
Implementar un sistema de an√°lisis inteligente de im√°genes que genere descripciones autom√°ticas, tags, categor√≠as y metadatos de im√°genes, permitiendo crear preguntas de trivia basadas en contenido visual de forma autom√°tica.

#### Necesidad (N):
Automatizar la generaci√≥n de contenido para preguntas visuales, mejorar la accesibilidad describiendo im√°genes, y permitir b√∫squeda y categorizaci√≥n autom√°tica de im√°genes por contenido.

#### Entidades (E):
- Servicio de an√°lisis de im√°genes (Azure Computer Vision)
- Endpoint backend para an√°lisis
- Frontend para subir y visualizar an√°lisis
- Base de datos para almacenar metadatos de im√°genes
- Sistema de generaci√≥n de preguntas basado en an√°lisis

#### Soporte (S):
- **Servicio:** Azure Computer Vision API Analyze Image
- **Endpoint Backend:** `POST /api/vision/analyze-image`
- **Frontend:** Componente para subir im√°genes y mostrar an√°lisis
- **Variables de Entorno:** 
  - `AZURE_COMPUTER_VISION_KEY`
  - `AZURE_COMPUTER_VISION_ENDPOINT`
- **Biblioteca:** `@azure/cognitiveservices-computervision` o HTTP REST

#### Suposiciones (S):
- Azure Computer Vision est√° configurado
- Las im√°genes contienen contenido reconocible
- Los usuarios tienen permisos para subir im√°genes
- El an√°lisis puede usarse para generar preguntas autom√°ticamente

#### Criterios de Aceptaci√≥n (A):

**1. Configuraci√≥n de Azure:**
- ‚úÖ Variable `AZURE_COMPUTER_VISION_KEY` en `.env`
- ‚úÖ Variable `AZURE_COMPUTER_VISION_ENDPOINT` en `.env`
- ‚úÖ Dependencias instaladas

**2. Endpoint Backend:**
- ‚úÖ Ruta `POST /api/vision/analyze-image` en `backend-v1/routes/vision.routes.js`
- ‚úÖ M√©todo `analyzeImage` en `visionController.js`
- ‚úÖ Autenticaci√≥n requerida
- ‚úÖ Validaci√≥n de imagen Base64 o archivo, m√°ximo 4MB

**3. Integraci√≥n con Azure Analyze Image:**
- ‚úÖ POST a `https://{endpoint}/vision/v3.2/analyze?visualFeatures=Description,Tags,Categories,Objects,Color`
- ‚úÖ Headers: `Ocp-Apim-Subscription-Key` y `Content-Type: application/octet-stream`
- ‚úÖ Par√°metros visuales: Description, Tags, Categories, Objects, Color
- ‚úÖ Procesamiento correcto de respuesta JSON

**4. Extracci√≥n de Datos:**
- ‚úÖ Descripci√≥n principal y descripciones alternativas
- ‚úÖ Tags con niveles de confianza
- ‚úÖ Categor√≠as detectadas (abstract, people, outdoor, etc.)
- ‚úÖ Objetos detectados con bounding boxes
- ‚úÖ Colores dominantes y acento de color
- ‚úÖ Metadatos: dimensiones, formato

**5. Respuesta del Endpoint:**
- ‚úÖ Objeto JSON estructurado con descripci√≥n, tags, categor√≠as, objetos y colores
- ‚úÖ Niveles de confianza incluidos
- ‚úÖ Tags y categor√≠as ordenados por confianza descendente

**6. Manejo de Errores:**
- ‚úÖ Error 400 con mensaje claro para imagen inv√°lida
- ‚úÖ Mensaje si no se detecta contenido reconocible
- ‚úÖ Manejo de 401, 429, 500 con mensajes apropiados
- ‚úÖ Manejo de timeouts con reintentos
- ‚úÖ Logging de errores

**7. Pruebas:**
- ‚úÖ Prueba unitaria con imagen de prueba
- ‚úÖ Prueba de integraci√≥n del endpoint
- ‚úÖ Prueba con im√°genes de arte, geograf√≠a, objetos, personas
- ‚úÖ Verificaci√≥n de manejo de errores

**8. Integraci√≥n con el Juego - Frontend:**
- ‚úÖ Componente `ImageAnalysisQuestionCreator.jsx` con:
  - Subida de imagen (drag & drop o bot√≥n)
  - Preview de imagen
  - Bot√≥n "Analizar Imagen" ‚Üí llamada a `/api/vision/analyze-image`
  - Spinner de carga
  - Resultados en secciones:
    - Descripci√≥n Principal (texto destacado)
    - Tags Detectados (chips/badges, confianza m√≠nima 0.7)
    - Categor√≠as (lista)
    - Objetos Detectados (lista con confianza)
- ‚úÖ Generaci√≥n Autom√°tica de Preguntas:
  - Bot√≥n "Generar Pregunta desde An√°lisis":
    1. Usa descripci√≥n como base para pregunta
    2. Usa tags para sugerir categor√≠a
    3. Pre-llena campo: "¬øQu√© se muestra en esta imagen?"
    4. Sugiere opciones basadas en tags y objetos
  - Usuario puede editar antes de guardar
- ‚úÖ Integraci√≥n con `AIQuestionGenerator`:
  - Opci√≥n "Crear desde An√°lisis de Imagen"
  - An√°lisis pre-llena formulario
  - Usuario completa/edita y crea pregunta
- ‚úÖ Flujo de Usuario Completo:
  1. Usuario va a "Crear Juego" o "Generar Preguntas"
  2. Selecciona "Crear desde An√°lisis de Imagen"
  3. Sube imagen (monumento, obra de arte, paisaje, objeto)
  4. Sistema analiza y muestra resultados
  5. Usuario revisa descripci√≥n, tags y objetos
  6. Clic en "Generar Pregunta"
  7. Sistema pre-llena formulario con pregunta sugerida
  8. Usuario edita pregunta y opciones
  9. Usuario selecciona respuesta correcta y crea pregunta
- ‚úÖ Resultado Final en el Juego:
  - Preguntas aparecen con imagen asociada
  - Durante juego, jugadores ven:
    - Imagen en la pregunta
    - Texto de pregunta generada desde an√°lisis
    - Opciones de respuesta
  - Ejemplo: "¬øQu√© monumento hist√≥rico se muestra en la imagen?" con opciones basadas en tags
  - Validaci√≥n de respuesta correcta normal

**9. Documentaci√≥n:**
- ‚úÖ Endpoint documentado en Swagger
- ‚úÖ Ejemplos de im√°genes y respuestas esperadas
- ‚úÖ Gu√≠a de integraci√≥n para generar preguntas

#### Tecnolog√≠as:
- Azure Computer Vision Analyze Image
- Node.js
- Express
- React

---

### HU-VC4: Detecci√≥n de Objetos en Im√°genes
**üìÖ Fecha Objetivo:** Viernes 21 Noviembre 2025  
**üî¢ Estimaci√≥n:** 10 Story Points  
**üéØ Prioridad:** Media  
**ü§ñ Requiere IA:** ‚úÖ S√≠ (Azure Computer Vision Object Detection)

#### Contexto (C):
Los usuarios de BrainBlitz necesitan crear preguntas visuales donde se identifiquen objetos espec√≠ficos en im√°genes. Por ejemplo, "¬øQu√© objeto aparece en esta imagen?" o "¬øCu√°ntos objetos de tipo X hay en la imagen?". Actualmente, no existe funcionalidad para detectar y localizar objetos en im√°genes.

#### Objetivo (O):
Implementar un sistema de detecci√≥n de objetos que identifique y localice objetos espec√≠ficos dentro de im√°genes, permitiendo crear preguntas interactivas basadas en la detecci√≥n de objetos y mejorar la experiencia de preguntas visuales.

#### Necesidad (N):
Habilitar la creaci√≥n de preguntas visuales interactivas, permitir b√∫squeda de objetos en im√°genes, y mejorar la accesibilidad describiendo qu√© objetos est√°n presentes en una imagen.

#### Entidades (E):
- Servicio de detecci√≥n de objetos (Azure Computer Vision)
- Endpoint backend para detecci√≥n
- Frontend para visualizar objetos detectados
- Base de datos para almacenar detecciones
- Sistema de preguntas basadas en objetos

#### Soporte (S):
- **Servicio:** Azure Computer Vision API Object Detection
- **Endpoint Backend:** `POST /api/vision/detect-objects`
- **Frontend:** Componente para mostrar objetos con bounding boxes
- **Variables de Entorno:** 
  - `AZURE_COMPUTER_VISION_KEY`
  - `AZURE_COMPUTER_VISION_ENDPOINT`
- **Biblioteca:** `@azure/cognitiveservices-computervision` o HTTP REST

#### Suposiciones (S):
- Azure Computer Vision soporta detecci√≥n de objetos
- Las im√°genes contienen objetos reconocibles
- Los usuarios necesitan crear preguntas basadas en objetos detectados

#### Criterios de Aceptaci√≥n (A):

**1. Configuraci√≥n de Azure:**
- ‚úÖ Variable `AZURE_COMPUTER_VISION_KEY` configurada
- ‚úÖ Variable `AZURE_COMPUTER_VISION_ENDPOINT` configurada
- ‚úÖ Librer√≠a de Azure instalada o HTTP REST

**2. Endpoint Backend:**
- ‚úÖ Ruta `POST /api/vision/detect-objects` en `backend-v1/routes/vision.routes.js`
- ‚úÖ M√©todo `detectObjects` en `visionController.js`
- ‚úÖ Autenticaci√≥n requerida
- ‚úÖ Validaci√≥n de imagen Base64 o archivo, m√°ximo 4MB

**3. Integraci√≥n con Azure Object Detection:**
- ‚úÖ POST a `https://{endpoint}/vision/v3.2/detect`
- ‚úÖ Headers: `Ocp-Apim-Subscription-Key` y `Content-Type: application/octet-stream`
- ‚úÖ Procesamiento de respuesta JSON con objetos detectados

**4. Extracci√≥n de Objetos:**
- ‚úÖ Lista de objetos con nombre, confianza, bounding box y √°rea
- ‚úÖ Filtrado opcional por confianza < 0.5
- ‚úÖ Ordenamiento por confianza descendente o √°rea

**5. Respuesta del Endpoint:**
- ‚úÖ Formato JSON con array de objetos, total y dimensiones de imagen
- ‚úÖ Metadatos: total de objetos y dimensiones
- ‚úÖ Bounding boxes en p√≠xeles o normalizados (0-1)

**6. Funcionalidades Adicionales:**
- ‚úÖ Par√°metro opcional `objectName` para buscar objeto espec√≠fico
- ‚úÖ Conteo de cada tipo de objeto
- ‚úÖ Agrupaci√≥n de objetos del mismo tipo

**7. Manejo de Errores:**
- ‚úÖ Error 400 con mensaje claro para imagen inv√°lida
- ‚úÖ Lista vac√≠a si no se detectan objetos (no error)
- ‚úÖ Manejo de errores de API con mensajes apropiados
- ‚úÖ Manejo de timeouts
- ‚úÖ Logging de errores

**8. Pruebas:**
- ‚úÖ Prueba unitaria con imagen de prueba
- ‚úÖ Prueba de integraci√≥n del endpoint
- ‚úÖ Verificaci√≥n de detecci√≥n de m√∫ltiples objetos
- ‚úÖ Verificaci√≥n de precisi√≥n
- ‚úÖ Verificaci√≥n de manejo de errores

**9. Integraci√≥n con el Juego - Frontend:**
- ‚úÖ Componente `ObjectDetectionQuestionCreator.jsx` con:
  - Subida de imagen (drag & drop o bot√≥n)
  - Preview de imagen
  - Bot√≥n "Detectar Objetos" ‚Üí llamada a `/api/vision/detect-objects`
  - Spinner de carga
  - Imagen con bounding boxes dibujados sobre objetos
  - Lista de objetos con:
    - Nombre del objeto
    - Nivel de confianza (barra o porcentaje)
    - Posici√≥n (coordenadas)
- ‚úÖ Visualizaci√≥n Interactiva:
  - Hover sobre objeto en lista ‚Üí resalta bounding box
  - Clic en bounding box ‚Üí resalta en lista
  - Filtro por confianza m√≠nima (slider)
  - Contador por tipo (ej: "3 guitarras", "1 persona", "2 sillas")
- ‚úÖ Generaci√≥n Autom√°tica de Preguntas:
  - Bot√≥n "Crear Pregunta de Objetos":
    1. Genera: "¬øQu√© objeto aparece en esta imagen?" o "¬øCu√°ntos [objeto] hay?"
    2. Usa objetos detectados como opciones
    3. Marca objeto con mayor confianza como correcto
    4. Pre-llena formulario
  - Opci√≥n para preguntas de conteo:
    - "¬øCu√°ntos [objeto] hay en la imagen?"
    - Sistema cuenta objetos del mismo tipo
    - Genera opciones num√©ricas (0, 1, 2, 3, 4+)
- ‚úÖ Integraci√≥n con `AIQuestionGenerator`:
  - Opci√≥n "Crear Pregunta de Detecci√≥n de Objetos"
  - Imagen con objetos se guarda asociada a pregunta
  - Formulario pre-llenado con pregunta y opciones
- ‚úÖ Flujo de Usuario Completo:
  1. Usuario va a "Crear Juego" o "Generar Preguntas"
  2. Selecciona "Crear Pregunta de Detecci√≥n de Objetos"
  3. Sube imagen con objetos (instrumentos, animales, objetos)
  4. Sistema detecta objetos y muestra resultados visuales
  5. Usuario revisa objetos y ajusta filtro de confianza
  6. Usuario selecciona tipo de pregunta:
     - "¬øQu√© objeto es este?" (identificaci√≥n)
     - "¬øCu√°ntos [objeto] hay?" (conteo)
  7. Sistema genera pregunta y opciones autom√°ticamente
  8. Usuario edita si es necesario
  9. Usuario confirma respuesta correcta y crea pregunta
- ‚úÖ Resultado Final en el Juego:
  - Preguntas muestran:
    - Imagen original (sin bounding boxes)
    - Texto de pregunta (ej: "¬øQu√© objeto musical aparece?")
    - Opciones de respuesta (objetos detectados)
  - Durante juego, jugadores:
    1. Ven imagen de pregunta
    2. Leen pregunta sobre objeto a identificar/contar
    3. Seleccionan respuesta entre opciones
    4. Sistema valida respuesta correcta
  - Ejemplo:
    - Imagen: Foto de guitarra, piano y viol√≠n
    - Pregunta: "¬øQu√© instrumento musical aparece en la imagen?"
    - Opciones: ["Guitarra", "Piano", "Viol√≠n", "Bater√≠a"]
    - Respuesta correcta: "Guitarra" (objeto con mayor confianza)

**10. Documentaci√≥n:**
- ‚úÖ Endpoint documentado en Swagger
- ‚úÖ Ejemplos visuales de im√°genes y objetos detectados
- ‚úÖ Gu√≠a de uso para crear preguntas

#### Tecnolog√≠as:
- Azure Computer Vision Object Detection
- Node.js
- Express
- React
- Canvas API (para dibujar bounding boxes)

---

## ü§ñ Clasificaci√≥n de HUs seg√∫n Uso de IA

### ‚úÖ Historias que REQUIEREN IA (4/4 - 100%)

Todas las historias de usuario en este proyecto utilizan servicios de Inteligencia Artificial:

| ID | Historia de Usuario | Servicio de IA | Modelo/Algoritmo |
|----|-------------------|----------------|------------------|
| **HU-VC1** | Reconocimiento Facial | DeepFace | VGG-Face (embeddings faciales) |
| **HU-VC2** | OCR - Extracci√≥n de Texto | Azure Computer Vision | OCR v3.2 |
| **HU-VC3** | An√°lisis de Im√°genes | Azure Computer Vision | Analyze Image API |
| **HU-VC4** | Detecci√≥n de Objetos | Azure Computer Vision | Object Detection API |

### üìä An√°lisis por Tipo de IA:

**1. Visi√≥n Computacional con Azure (3 HUs):**
- HU-VC2: OCR para extracci√≥n de texto
- HU-VC3: An√°lisis inteligente de im√°genes (descripci√≥n, tags, categor√≠as)
- HU-VC4: Detecci√≥n y localizaci√≥n de objetos

**2. Reconocimiento Facial con DeepFace (1 HU):**
- HU-VC1: Autenticaci√≥n biom√©trica mediante reconocimiento facial

### üéØ Distribuci√≥n de Story Points por IA:

- **DeepFace (Reconocimiento Facial):** 13 SP (31.7%)
- **Azure OCR:** 8 SP (19.5%)
- **Azure Analyze Image:** 10 SP (24.4%)
- **Azure Object Detection:** 10 SP (24.4%)

**Total:** 41 Story Points implementando servicios de IA

---

## üìÖ Release Plan

### üöÄ Release 1.0: "Computer Vision Foundation"
**Fecha de Lanzamiento:** 24 Noviembre 2025  
**Duraci√≥n:** 2 Sprints (8 d√≠as laborables)

---

### Sprint 1: "Autenticaci√≥n y Extracci√≥n" (17-19 Noviembre 2025)
**Objetivo:** Implementar funcionalidades core de autenticaci√≥n biom√©trica y procesamiento de texto

#### üì¶ Entregables:

**Lunes 17 Noviembre:**
- ‚úÖ **HU-VC2: OCR - Extracci√≥n de Texto** (8 SP)
  - Endpoint `/api/vision/extract-text` funcional
  - Integraci√≥n con Azure Computer Vision OCR
  - Componente frontend `OCRQuestionCreator.jsx`
  - Flujo completo de creaci√≥n de preguntas desde im√°genes con texto
  - Pruebas unitarias e integraci√≥n
  - Documentaci√≥n Swagger

**Martes 18 Noviembre:**
- ‚úÖ **HU-VC1: Reconocimiento Facial** (13 SP)
  - Microservicio `facial-service` con DeepFace desplegado en Azure
  - Endpoints `/api/face/register` y `/api/face/login`
  - Componentes frontend `FaceRegister.jsx` y `FaceLogin.jsx`
  - Integraci√≥n con Firebase Auth
  - Sistema de embeddings faciales
  - Pruebas de seguridad y autenticaci√≥n
  - Documentaci√≥n completa

**Mi√©rcoles 19 Noviembre:**
- üîß Testing y refinamiento del Sprint 1
- üìù Documentaci√≥n de usuario
- üêõ Bug fixing
- üé® Mejoras de UX/UI

#### üìà M√©tricas del Sprint 1:
- **Story Points:** 21 SP
- **Historias Completadas:** 2
- **Endpoints API Nuevos:** 3
- **Componentes Frontend Nuevos:** 3
- **Servicios de IA Integrados:** 2 (DeepFace, Azure OCR)

---

### Sprint 2: "An√°lisis Visual Avanzado" (20-24 Noviembre 2025)
**Objetivo:** Implementar an√°lisis inteligente y detecci√≥n de objetos para preguntas visuales

#### üì¶ Entregables:

**Jueves 20 Noviembre:**
- ‚úÖ **HU-VC3: An√°lisis Inteligente de Im√°genes** (10 SP)
  - Endpoint `/api/vision/analyze-image` funcional
  - Integraci√≥n con Azure Computer Vision Analyze
  - Componente frontend `ImageAnalysisQuestionCreator.jsx`
  - Extracci√≥n de descripci√≥n, tags, categor√≠as, objetos y colores
  - Generaci√≥n autom√°tica de preguntas desde an√°lisis
  - Integraci√≥n con `AIQuestionGenerator`
  - Pruebas con diferentes tipos de im√°genes
  - Documentaci√≥n y ejemplos

**Viernes 21 Noviembre:**
- ‚úÖ **HU-VC4: Detecci√≥n de Objetos** (10 SP)
  - Endpoint `/api/vision/detect-objects` funcional
  - Integraci√≥n con Azure Object Detection
  - Componente frontend `ObjectDetectionQuestionCreator.jsx`
  - Visualizaci√≥n de bounding boxes
  - Generaci√≥n de preguntas de identificaci√≥n y conteo
  - Funcionalidades interactivas (hover, filtros)
  - Pruebas de precisi√≥n
  - Documentaci√≥n completa

**S√°bado-Domingo 22-23 Noviembre:**
- üîß Testing integral de todas las funcionalidades
- üìù Documentaci√≥n de usuario final
- üêõ Bug fixing y optimizaci√≥n
- üé® Refinamiento de UX/UI
- üîí Revisi√≥n de seguridad

**Lunes 24 Noviembre:**
- üöÄ **Despliegue a Producci√≥n**
- ‚úÖ Validaci√≥n de todas las funcionalidades en producci√≥n
- üìä Configuraci√≥n de monitoring y alertas
- üì¢ Anuncio de Release 1.0

#### üìà M√©tricas del Sprint 2:
- **Story Points:** 20 SP
- **Historias Completadas:** 2
- **Endpoints API Nuevos:** 2
- **Componentes Frontend Nuevos:** 2
- **Servicios de IA Integrados:** 1 (Azure Computer Vision - 2 APIs)

---

### üìä Resumen General del Release 1.0

#### Story Points Totales: 41 SP
- Sprint 1: 21 SP (51.2%)
- Sprint 2: 20 SP (48.8%)

#### Componentes Entregados:
- **Backend:**
  - 5 Endpoints API nuevos
  - 1 Microservicio de reconocimiento facial
  - 5 Controladores nuevos
  - Sistema de validaci√≥n y manejo de errores
  
- **Frontend:**
  - 5 Componentes React nuevos
  - Integraci√≥n con c√°mara web
  - Visualizaci√≥n de an√°lisis de IA
  - Sistema de generaci√≥n autom√°tica de preguntas
  
- **Integraciones de IA:**
  - DeepFace (VGG-Face)
  - Azure Computer Vision OCR
  - Azure Computer Vision Analyze
  - Azure Computer Vision Object Detection
  
- **Infraestructura:**
  - Azure Container Instances (microservicio facial)
  - Firebase Firestore (embeddings faciales)
  - Azure Cognitive Services (3 APIs)

#### Capacidades Nuevas para Usuarios:
1. ‚úÖ Autenticaci√≥n sin contrase√±a mediante rostro
2. ‚úÖ Creaci√≥n de preguntas desde im√°genes con texto
3. ‚úÖ Generaci√≥n autom√°tica de preguntas desde an√°lisis de im√°genes
4. ‚úÖ Creaci√≥n de preguntas visuales con detecci√≥n de objetos
5. ‚úÖ Mejora de accesibilidad con descripci√≥n autom√°tica de im√°genes

---

### üîÑ Post-Release Activities (25-30 Noviembre 2025)

**Lunes 25 Noviembre:**
- üìä An√°lisis de m√©tricas de uso
- üìù Recolecci√≥n de feedback de usuarios
- üêõ Identificaci√≥n de bugs cr√≠ticos

**Martes 26 - Viernes 29 Noviembre:**
- üîß Hotfixes seg√∫n prioridad
- üìà Optimizaci√≥n de rendimiento
- üé® Mejoras de UX basadas en feedback
- üìö Actualizaci√≥n de documentaci√≥n

**S√°bado 30 Noviembre:**
- üìä Reporte final de Release 1.0
- üéØ Planificaci√≥n de Release 2.0
- üèÜ Retrospectiva del proyecto

---

## üèóÔ∏è Arquitectura T√©cnica

### Arquitectura General del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        FRONTEND (React)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ FaceRegister ‚îÇ ‚îÇ  FaceLogin   ‚îÇ ‚îÇ OCRQuestionCreator   ‚îÇ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ                      ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ImageAnalysisQuestionCreator‚îÇ ‚îÇObjectDetectionCreator    ‚îÇ‚îÇ
‚îÇ  ‚îÇ                            ‚îÇ ‚îÇ                          ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ HTTPS/REST API
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND (Node.js/Express)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ /face/       ‚îÇ ‚îÇ /vision/     ‚îÇ ‚îÇ /vision/             ‚îÇ‚îÇ
‚îÇ  ‚îÇ  register    ‚îÇ ‚îÇ  extract-text‚îÇ ‚îÇ  analyze-image       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  login       ‚îÇ ‚îÇ              ‚îÇ ‚îÇ                      ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ  ‚îÇ /vision/detect-objects       ‚îÇ                          ‚îÇ
‚îÇ  ‚îÇ                              ‚îÇ                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                    ‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚ñº                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FACIAL SERVICE     ‚îÇ        ‚îÇ   AZURE COGNITIVE SERVICES   ‚îÇ
‚îÇ  (DeepFace)         ‚îÇ        ‚îÇ                              ‚îÇ
‚îÇ                     ‚îÇ        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ        ‚îÇ  ‚îÇ Computer Vision OCR    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ VGG-Face      ‚îÇ  ‚îÇ        ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Embeddings    ‚îÇ  ‚îÇ        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                     ‚îÇ        ‚îÇ  ‚îÇ Analyze Image API      ‚îÇ  ‚îÇ
‚îÇ  Azure Container    ‚îÇ        ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ
‚îÇ  Instances          ‚îÇ        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
         ‚îÇ                     ‚îÇ  ‚îÇ Object Detection API   ‚îÇ  ‚îÇ
         ‚îÇ                     ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ
         ‚ñº                     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  FIREBASE           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Auth          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Firestore     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Embeddings)  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos por Funcionalidad

#### 1. Reconocimiento Facial (HU-VC1)

**Registro:**
```
Usuario ‚Üí C√°mara Web ‚Üí Base64 ‚Üí /api/face/register
    ‚Üí Facial Service (DeepFace) ‚Üí Embedding VGG-Face
    ‚Üí Firebase Firestore ‚Üí Confirmaci√≥n
```

**Login:**
```
Usuario ‚Üí C√°mara Web ‚Üí Base64 + Email ‚Üí /api/face/login
    ‚Üí Buscar Usuario en Firebase Auth
    ‚Üí Obtener Embedding Almacenado
    ‚Üí Facial Service (Comparaci√≥n) ‚Üí Verificaci√≥n
    ‚Üí Generar Custom Token ‚Üí Autenticaci√≥n Exitosa
```

#### 2. OCR - Extracci√≥n de Texto (HU-VC2)

```
Usuario ‚Üí Imagen ‚Üí Base64 ‚Üí /api/vision/extract-text
    ‚Üí Azure Computer Vision OCR API
    ‚Üí Procesamiento As√≠ncrono (analyze ‚Üí results)
    ‚Üí Extracci√≥n de Texto por L√≠neas
    ‚Üí Texto Limpio + Metadatos ‚Üí Usuario
    ‚Üí Pre-llenar Formulario de Pregunta
```

#### 3. An√°lisis de Im√°genes (HU-VC3)

```
Usuario ‚Üí Imagen ‚Üí Base64 ‚Üí /api/vision/analyze-image
    ‚Üí Azure Computer Vision Analyze API
    ‚Üí Extracci√≥n: Description, Tags, Categories, Objects, Colors
    ‚Üí Procesamiento y Ordenamiento por Confianza
    ‚Üí Resultados Estructurados ‚Üí Usuario
    ‚Üí Generaci√≥n Autom√°tica de Pregunta
    ‚Üí Pre-llenar Formulario con Sugerencias
```

#### 4. Detecci√≥n de Objetos (HU-VC4)

```
Usuario ‚Üí Imagen ‚Üí Base64 ‚Üí /api/vision/detect-objects
    ‚Üí Azure Computer Vision Object Detection API
    ‚Üí Detecci√≥n de Objetos + Bounding Boxes
    ‚Üí Filtrado por Confianza
    ‚Üí Agrupaci√≥n por Tipo
    ‚Üí Visualizaci√≥n con Bounding Boxes ‚Üí Usuario
    ‚Üí Generaci√≥n de Pregunta (Identificaci√≥n o Conteo)
    ‚Üí Pre-llenar Formulario
```

### Stack Tecnol√≥gico Completo

#### Backend:
- **Runtime:** Node.js v18+
- **Framework:** Express.js
- **Autenticaci√≥n:** Firebase Admin SDK
- **Base de Datos:** Firebase Firestore
- **Validaci√≥n:** express-validator
- **HTTP Client:** axios / node-fetch
- **Procesamiento de Im√°genes:** Sharp (opcional)

#### Frontend:
- **Framework:** React 18+
- **Routing:** React Router v6
- **State Management:** Context API / Redux (opcional)
- **Styling:** TailwindCSS
- **HTTP Client:** axios
- **Media:** getUserMedia API (WebRTC)
- **Canvas:** HTML5 Canvas (para bounding boxes)

#### Servicios de IA:
- **DeepFace:** Python-based facial recognition library
- **Azure Computer Vision:** v3.2
  - OCR (Read API)
  - Analyze Image
  - Object Detection
- **Modelo Facial:** VGG-Face (embeddings de 128 dimensiones)

#### Infraestructura:
- **Hosting Backend:** Azure App Service / Cloud Run
- **Hosting Frontend:** Vercel / Netlify
- **Container Registry:** Azure Container Registry
- **Container Instances:** Azure Container Instances (facial-service)
- **Storage:** Firebase Storage (im√°genes)
- **CI/CD:** GitHub Actions

#### DevOps:
- **Version Control:** Git + GitHub
- **CI/CD:** GitHub Actions (workflow automatizado)
- **Containers:** Docker
- **Monitoring:** Azure Application Insights
- **Logs:** Winston / Morgan

---

## ‚öôÔ∏è Configuraci√≥n del Proyecto

### Prerequisitos

```bash
# Node.js v18+
node --version

# npm v9+
npm --version

# Git
git --version

# Docker (para facial-service)
docker --version
```

### Variables de Entorno (.env)

```bash
# ==========================================
# FIREBASE CONFIGURATION
# ==========================================
FIREBASE_PROJECT_ID=your-project-id
FIREBASE_PRIVATE_KEY=your-private-key
FIREBASE_CLIENT_EMAIL=your-client-email

# ==========================================
# AZURE COMPUTER VISION
# ==========================================
AZURE_COMPUTER_VISION_KEY=your-azure-cv-key
AZURE_COMPUTER_VISION_ENDPOINT=https://your-region.api.cognitive.microsoft.com/

# ==========================================
# DEEPFACE FACIAL SERVICE
# ==========================================
DEEPFACE_SERVICE_URL=https://your-facial-service.azurecontainer.io

# ==========================================
# SERVER CONFIGURATION
# ==========================================
PORT=3000
NODE_ENV=production

# ==========================================
# CORS
# ==========================================
FRONTEND_URL=https://your-frontend-domain.com

# ==========================================
# RATE LIMITING
# ==========================================
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100
```

### Instalaci√≥n Backend

```bash
# Clonar repositorio
git clone https://github.com/your-org/brainblitz.git
cd brainblitz/backend-v1

# Instalar dependencias
npm install

# Instalar dependencias espec√≠ficas de Azure
npm install @azure/cognitiveservices-computervision @azure/ms-rest-js

# Instalar dependencias de Firebase
npm install firebase-admin

# Copiar y configurar variables de entorno
cp .env.example .env
# Editar .env con tus credenciales

# Ejecutar en desarrollo
npm run dev

# Ejecutar en producci√≥n
npm start
```

### Instalaci√≥n Frontend

```bash
cd brainblitz/frontend-v2

# Instalar dependencias
npm install

# Instalar dependencias espec√≠ficas
npm install axios react-router-dom

# Copiar y configurar variables de entorno
cp .env.example .env
# Configurar REACT_APP_API_URL

# Ejecutar en desarrollo
npm start

# Build para producci√≥n
npm run build
```

### Despliegue Facial Service (Docker)

```bash
cd brainblitz/facial-service

# Build imagen Docker
docker build -t facial-service:latest .

# Ejecutar localmente (testing)
docker run -p 5000:5000 facial-service:latest

# Push a Azure Container Registry
az acr login --name yourregistryname
docker tag facial-service:latest yourregistryname.azurecr.io/facial-service:latest
docker push yourregistryname.azurecr.io/facial-service:latest

# Desplegar en Azure Container Instances
az container create \
  --resource-group your-resource-group \
  --name facial-service \
  --image yourregistryname.azurecr.io/facial-service:latest \
  --dns-name-label facial-service-unique \
  --ports 5000
```

### Configuraci√≥n de Azure Computer Vision

1. **Crear Recurso en Azure Portal:**
   - Ir a Azure Portal ‚Üí Create Resource
   - Buscar "Computer Vision"
   - Crear recurso en regi√≥n deseada
   - Obtener Key y Endpoint

2. **Configurar Variables de Entorno:**
   ```bash
   AZURE_COMPUTER_VISION_KEY=your-key-here
   AZURE_COMPUTER_VISION_ENDPOINT=https://your-region.api.cognitive.microsoft.com/
   ```

3. **Verificar Conectividad:**
   ```bash
   curl -X POST "https://your-region.api.cognitive.microsoft.com/vision/v3.2/analyze?visualFeatures=Description" \
     -H "Ocp-Apim-Subscription-Key: your-key" \
     -H "Content-Type: application/octet-stream" \
     --data-binary @test-image.jpg
   ```

### GitHub Actions Workflow

El proyecto incluye un workflow automatizado que:

1. ‚úÖ Crea el proyecto "Product Backlog" en GitHub Projects
2. ‚úÖ Crea etiquetas de prioridad (Alta, Media, Baja)
3. ‚úÖ Crea milestones para cada sprint
4. ‚úÖ Crea todas las issues (HUs) con descripci√≥n completa
5. ‚úÖ A√±ade las issues al proyecto autom√°ticamente
6. ‚úÖ Mueve las issues a la columna "Todo"
7. ‚úÖ Crea ramas de trabajo para cada issue
8. ‚úÖ Vincula ramas con issues mediante comentarios

**Para ejecutar:**
```bash
# Ir a GitHub ‚Üí Actions ‚Üí "üöÄ Crear Backlog y Sprints del Proyecto"
# Click en "Run workflow"
```

---

## üìö Documentaci√≥n Adicional

### Endpoints API

Documentaci√≥n completa disponible en:
- **Swagger UI:** `https://api.brainblitz.com/api-docs`
- **Postman Collection:** Disponible en `/docs/postman`

### Testing

```bash
# Ejecutar tests unitarios
npm test

# Ejecutar tests de integraci√≥n
npm run test:integration

# Generar reporte de cobertura
npm run test:coverage
```

### Contribuci√≥n

Para contribuir al proyecto:

1. Fork el repositorio
2. Crear rama desde `main`: `git checkout -b feature/nueva-funcionalidad`
3. Hacer cambios y commit: `git commit -m "feat: descripci√≥n"`
4. Push a tu fork: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request hacia `main`

### Soporte

- **Issues:** https://github.com/your-org/brainblitz/issues
- **Discussions:** https://github.com/your-org/brainblitz/discussions
- **Email:** support@brainblitz.com

---

## üìÑ Licencia

Este proyecto est√° licenciado bajo MIT License - ver archivo [LICENSE](LICENSE) para detalles.

---

## üë• Equipo

- **Product Owner:** [Nombre]
- **Scrum Master:** [Nombre]
- **Desarrolladores:**
  - Backend: [Nombres]
  - Frontend: [Nombres]
  - DevOps: [Nombres]

---

**√öltima Actualizaci√≥n:** 15 Noviembre 2025  
**Versi√≥n del Documento:** 1.0  
**Estado del Proyecto:** En Desarrollo Activo üöÄ

