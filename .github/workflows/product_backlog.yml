name: ðŸš€ Crear Backlog y Sprints del Proyecto - VisiÃ³n Computacional

on:
  workflow_dispatch: # Permite ejecutar este workflow manualmente desde la pestaÃ±a Actions

# Define el token con permisos de escritura
permissions:
  contents: write # âš ï¸ Se cambiÃ³ a 'write' para poder crear ramas
  issues: write
  # Es necesario aÃ±adir permisos para 'pull-requests: write' para que pueda crear etiquetas
  pull-requests: write

jobs:
  setup-project-backlog:
    runs-on: ubuntu-latest
    steps:
      # PASO 1: Descarga el cÃ³digo del repositorio
      - name: Checkout repository
        uses: actions/checkout@v4

      # PASO 2 (MODIFICADO): Crea el proyecto si no existe y obtiene su nÃºmero automÃ¡ticamente
      - name: Crear Proyecto de GitHub si no existe
        id: project_setup # Le damos un ID a este paso para usar sus resultados
        env:
          # âš ï¸ MODIFICACIÃ“N AÃ‘ADIDA: Usa el Token de Acceso Personal para tener permisos
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
        run: |
          PROJECT_NAME="Product Backlog"
          # âš ï¸ MODIFICACIÃ“N AÃ‘ADIDA: Comando corregido para buscar el proyecto
          PROJECT_NUMBER=$(gh project list --owner "@me" --format json | jq -r ".projects[] | select(.title == \"$PROJECT_NAME\") | .number")
          
          if [ -z "$PROJECT_NUMBER" ]; then
            echo "Proyecto '$PROJECT_NAME' no encontrado. CreÃ¡ndolo ahora..."
            # Crea el proyecto y extrae su nÃºmero de la URL resultante
            PROJECT_URL=$(gh project create --owner "@me" --title "$PROJECT_NAME")
            PROJECT_NUMBER=$(echo "$PROJECT_URL" | rev | cut -d'/' -f1 | rev)
            echo "Proyecto creado con el nÃºmero: $PROJECT_NUMBER"
          else
            echo "Proyecto '$PROJECT_NAME' ya existe con el nÃºmero: $PROJECT_NUMBER"
          fi
          # Guarda el nÃºmero del proyecto para que otros pasos puedan usarlo
          echo "PROJECT_NUMBER=$PROJECT_NUMBER" >> $GITHUB_OUTPUT

      # PASO 3: Crea las Etiquetas de Prioridad si no existen
      - name: Crear Etiquetas de Prioridad si no existen
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }} # Usamos el PAT para consistencia
        run: |
          echo "Creando etiquetas de prioridad..."
          gh label create "Prioridad: Alta" --color "D93F0B" --description "Esta tarea es crÃ­tica y bloquea otras." || echo "La etiqueta 'Prioridad: Alta' ya existe."
          gh label create "Prioridad: Media" --color "FBCA04" --description "Tarea importante pero no urgente." || echo "La etiqueta 'Prioridad: Media' ya existe."
          gh label create "Prioridad: Baja" --color "0E8A16" --description "Tarea menor o mejora deseable." || echo "La etiqueta 'Prioridad: Baja' ya existe."
      
      # PASO 4: Crea los Milestones (Sprints) solo si no existen
      - name: Crear Milestones si no existen
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }} # Usamos el PAT para consistencia
          GH_REPO: ${{ github.repository }}
        run: |
          # --- Sprint Semana 1: 17-19 Noviembre ---
          MILESTONE_W1_TITLE="Sprint VisiÃ³n Computacional - Semana 1 (17-19 Nov 2025)"
          if gh api /repos/${GH_REPO}/milestones --paginate -q ".[] | select(.title == \"$MILESTONE_W1_TITLE\")" | grep -qF "$MILESTONE_W1_TITLE"; then
            echo "Milestone '$MILESTONE_W1_TITLE' ya existe. Omitiendo."
          else
            echo "Creando milestone '$MILESTONE_W1_TITLE'..."
            gh api --method POST -H "Accept: application/vnd.github+json" "/repos/${GH_REPO}/milestones" \
              -f title="$MILESTONE_W1_TITLE" \
              -f description='ImplementaciÃ³n de funcionalidades de visiÃ³n computacional con Azure. Semana 1: OCR y Reconocimiento Facial.' \
              -f due_on='2025-11-19T23:59:59Z'
          fi

          # --- Sprint Semana 2: 20-24 Noviembre ---
          MILESTONE_W2_TITLE="Sprint VisiÃ³n Computacional - Semana 2 (20-24 Nov 2025)"
          if gh api /repos/${GH_REPO}/milestones --paginate -q ".[] | select(.title == \"$MILESTONE_W2_TITLE\")" | grep -qF "$MILESTONE_W2_TITLE"; then
            echo "Milestone '$MILESTONE_W2_TITLE' ya existe. Omitiendo."
          else
            echo "Creando milestone '$MILESTONE_W2_TITLE'..."
            gh api --method POST -H "Accept: application/vnd.github+json" "/repos/${GH_REPO}/milestones" \
              -f title="$MILESTONE_W2_TITLE" \
              -f description='ImplementaciÃ³n de funcionalidades de visiÃ³n computacional con Azure. Semana 2: AnÃ¡lisis de ImÃ¡genes y DetecciÃ³n de Objetos.' \
              -f due_on='2025-11-24T23:59:59Z'
          fi

      # PASO 5: Crea todos los Issues, los aÃ±ade al proyecto y solo si no existen
      - name: Crear Issues y aÃ±adirlas al Proyecto
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }} # Usamos el PAT para consistencia
          PROJECT_NUMBER: ${{ steps.project_setup.outputs.PROJECT_NUMBER }}
        run: |
          # La funciÃ³n ahora tambiÃ©n aÃ±ade la issue al proyecto.
          # Acepta body como archivo o texto directo
          create_and_add_to_project() {
            local title="$1"
            local body_source="$2"  # Puede ser archivo o texto
            local label="$3"
            local milestone="$4"
            
            # Si body_source es un archivo, leerlo; si no, usarlo directamente
            if [ -f "$body_source" ]; then
              local body_file="$body_source"
            else
              # Crear archivo temporal con el texto
              local body_file=$(mktemp)
              echo "$body_source" > "$body_file"
            fi
            
            ISSUE_URL=$(gh issue list --state all --search "in:title \"$title\"" --json url -q ".[0].url")

            if [ -n "$ISSUE_URL" ]; then
              echo "Issue '$title' ya existe. Omitiendo creaciÃ³n."
            else
              echo "Creando issue '$title'..."
              ISSUE_URL=$(gh issue create --title "$title" --body-file "$body_file" --label "$label" --milestone "$milestone")
            fi
            
            # Limpiar archivo temporal si lo creamos
            if [ ! -f "$body_source" ] && [ -f "$body_file" ]; then
              rm -f "$body_file"
            fi

            if [ -n "$ISSUE_URL" ]; then
                echo "AÃ±adiendo issue $ISSUE_URL al proyecto ${{ env.PROJECT_NUMBER }}..."
                PROJECT_ID=$(gh project view ${{ env.PROJECT_NUMBER }} --owner "@me" --format json | jq -r '.id')
                CONTENT_ID=$(gh issue view $ISSUE_URL --json id | jq -r '.id')

                # AÃ±ade la issue al proyecto y obtiene el ID del item
                ITEM_ID=$(gh api graphql -f query='
                  mutation($project:ID!, $content:ID!) {
                    addProjectV2ItemById(input: {projectId: $project, contentId: $content}) {
                      item { id }
                    }
                  }
                ' -f project="$PROJECT_ID" -f content="$CONTENT_ID" | jq -r '.data.addProjectV2ItemById.item.id')

                # === MOVER LA ISSUE A LA COLUMNA 'Todo' ===
                TODO_COLUMN_ID="f75ad846"
                STATUS_FIELD_ID="PVTSSF_lAHOCfyr_s4BFLg_zg2l7HM"
                # Actualiza el campo Status del item usando una query mÃ¡s simple
                echo "Moviendo issue a Todo con IDs: PROJECT=$PROJECT_ID, ITEM=$ITEM_ID, FIELD=$STATUS_FIELD_ID, OPTION=$TODO_COLUMN_ID"
                
                # Usar una query GraphQL mÃ¡s simple sin variables complejas
                gh api graphql -f query="
                  mutation {
                    updateProjectV2ItemFieldValue(input: {
                      projectId: \"$PROJECT_ID\",
                      itemId: \"$ITEM_ID\",
                      fieldId: \"$STATUS_FIELD_ID\",
                      value: {singleSelectOptionId: \"$TODO_COLUMN_ID\"}
                    }) {
                      projectV2Item { id }
                    }
                  }
                " || echo "No se pudo mover la issue a la columna 'Todo'. Revisa el ID."
            fi
          }

          # ===============================================
          # ==  HU: VISIÃ“N COMPUTACIONAL (AZURE)         ==
          # ==  Organizadas por EstimaciÃ³n y Prioridad  ==
          # ==  Fechas: 17-24 Noviembre 2025            ==
          # ===============================================

          # HU 1: OCR - ExtracciÃ³n de Texto (8 SP, Media) - Lunes 17 Nov
          OCR_BODY_FILE=$(mktemp)
          cat > "$OCR_BODY_FILE" << 'EOFBODY1'
**C (Contexto)**: Los usuarios y administradores de BrainBlitz necesitan una forma de convertir imÃ¡genes con texto (pantallas, documentos, carteles, capturas) en texto editable para generar preguntas automÃ¡ticamente o procesar contenido visual. Actualmente, el sistema requiere que las preguntas se ingresen manualmente, lo cual es lento y propenso a errores.

            **O (Objetivo)**: Implementar un sistema de reconocimiento Ã³ptico de caracteres (OCR) que permita extraer texto de imÃ¡genes subidas por usuarios o administradores, facilitando la creaciÃ³n de preguntas y el procesamiento de contenido visual.

            **N (Necesidad)**: Automatizar la extracciÃ³n de texto de imÃ¡genes para reducir el tiempo de creaciÃ³n de preguntas, permitir que usuarios suban imÃ¡genes con preguntas y convertirlas automÃ¡ticamente, y mejorar la accesibilidad del contenido visual.

            **E (Entidad)**: Servicio de OCR (Azure Computer Vision), endpoint backend para procesamiento de imÃ¡genes, frontend para subir imÃ¡genes, base de datos para almacenar texto extraÃ­do, sistema de validaciÃ³n y limpieza de texto.

            **S (Soporte)**: 
            - Azure Computer Vision API con servicio OCR
            - Endpoint backend: \`POST /api/vision/extract-text\`
            - Frontend: componente para subir imÃ¡genes y mostrar texto extraÃ­do
            - Variable de entorno: \`AZURE_COMPUTER_VISION_KEY\` y \`AZURE_COMPUTER_VISION_ENDPOINT\`
            - Biblioteca: \`@azure/cognitiveservices-computervision\` o llamadas HTTP REST directas

            **S (SuposiciÃ³n)**: 
            - Azure Computer Vision estÃ¡ configurado y tiene crÃ©ditos disponibles
            - Las imÃ¡genes subidas contienen texto legible
            - Los usuarios tienen permisos para subir imÃ¡genes
            - El texto extraÃ­do puede requerir limpieza y validaciÃ³n

            **A (Criterios de AceptaciÃ³n)**:

            #### **1. ConfiguraciÃ³n de Azure:**
            - âœ… **Cuenta Azure**: Existe una cuenta de Azure con Computer Vision habilitado
            - âœ… **API Key**: Se tiene la clave API de Azure Computer Vision
            - âœ… **Endpoint**: Se tiene la URL del endpoint de Azure Computer Vision
            - âœ… **Variables de Entorno**: Las variables \`AZURE_COMPUTER_VISION_KEY\` y \`AZURE_COMPUTER_VISION_ENDPOINT\` estÃ¡n configuradas en \`.env\`
            - âœ… **InstalaciÃ³n de Dependencias**: Se instala \`@azure/cognitiveservices-computervision\` o se usa \`axios\`/\`fetch\` para llamadas REST

            #### **2. Endpoint Backend:**
            - âœ… **Ruta**: Existe \`POST /api/vision/extract-text\` en \`backend-v1/routes/vision.routes.js\`
            - âœ… **Controlador**: Existe \`visionController.js\` con mÃ©todo \`extractText\`
            - âœ… **AutenticaciÃ³n**: El endpoint requiere autenticaciÃ³n (middleware \`authenticate.js\`)
            - âœ… **ValidaciÃ³n**: Valida que se reciba \`image\` en Base64 o archivo
            - âœ… **LÃ­mite de TamaÃ±o**: Valida que la imagen no exceda 4MB (lÃ­mite de Azure)

            #### **3. IntegraciÃ³n con Azure OCR:**
            - âœ… **ConversiÃ³n de Imagen**: Convierte Base64 a buffer binario para enviar a Azure
            - âœ… **Llamada a Azure**: Hace POST a \`https://{endpoint}/vision/v3.2/read/analyze\`
            - âœ… **Headers Correctos**: Incluye \`Ocp-Apim-Subscription-Key\` y \`Content-Type: application/octet-stream\`
            - âœ… **Procesamiento AsÃ­ncrono**: Maneja el flujo asÃ­ncrono de Azure (analyze â†’ get results)
            - âœ… **ExtracciÃ³n de Texto**: Extrae todas las lÃ­neas de texto de la respuesta de Azure
            - âœ… **Formato de Respuesta**: Retorna texto limpio y estructurado

            #### **4. Respuesta del Endpoint:**
            - âœ… **Formato JSON**: Retorna \`{ success: true, text: string, language: string, confidence: number, lines: array }\`
            - âœ… **Texto Completo**: Incluye todo el texto extraÃ­do concatenado
            - âœ… **LÃ­neas Individuales**: Incluye array con cada lÃ­nea de texto detectada
            - âœ… **Idioma Detectado**: Incluye el idioma detectado por Azure (es, en, etc.)
            - âœ… **Nivel de Confianza**: Incluye el nivel de confianza promedio de la extracciÃ³n

            #### **5. Manejo de Errores:**
            - âœ… **Imagen InvÃ¡lida**: Retorna error 400 si la imagen no es vÃ¡lida
            - âœ… **Sin Texto Detectado**: Retorna mensaje claro si no se detecta texto en la imagen
            - âœ… **Error de Azure**: Maneja errores de API de Azure (401, 429, 500) con mensajes claros
            - âœ… **Timeout**: Maneja timeouts de Azure con reintentos o mensaje de error apropiado
            - âœ… **Logging**: Registra errores en consola para debugging

            #### **6. Pruebas:**
            - âœ… **Prueba Unitaria**: Prueba la funciÃ³n de extracciÃ³n de texto con imagen de prueba
            - âœ… **Prueba de IntegraciÃ³n**: Prueba el endpoint completo con imagen real
            - âœ… **Prueba de Errores**: Prueba manejo de errores (imagen invÃ¡lida, sin texto, error de Azure)
            - âœ… **Prueba Manual**: Verifica que funciona con imÃ¡genes de diferentes tipos (JPG, PNG, PDF)

            #### **7. IntegraciÃ³n con el Juego - Frontend:**
            - âœ… **Componente OCR**: Existe componente \`OCRQuestionCreator.jsx\` en \`frontend-v2/src/components/\` que:
              - Permite subir una imagen (drag & drop o botÃ³n de selecciÃ³n)
              - Muestra preview de la imagen subida
              - BotÃ³n \"Extraer Texto\" que llama al endpoint \`/api/vision/extract-text\`
              - Muestra spinner de carga mientras procesa
              - Muestra el texto extraÃ­do en un textarea editable
              - BotÃ³n \"Usar como Pregunta\" que pre-llena el formulario de creaciÃ³n de preguntas
            - âœ… **IntegraciÃ³n con AIQuestionGenerator**: El componente OCR se integra con \`AIQuestionGenerator.jsx\`:
              - El texto extraÃ­do se pasa automÃ¡ticamente al campo de pregunta
              - El usuario puede editar el texto antes de crear la pregunta
              - El usuario completa las opciones y selecciona la respuesta correcta
              - Se crea la pregunta normalmente usando el flujo existente
            - âœ… **Flujo de Usuario Completo**:
              1. Usuario va a \"Crear Juego\" o \"Generar Preguntas\"
              2. Ve opciÃ³n \"Crear desde Imagen con Texto\" (nueva pestaÃ±a o botÃ³n)
              3. Sube imagen con texto (ej: captura de pantalla, foto de libro, documento)
              4. Sistema extrae texto automÃ¡ticamente
              5. Texto aparece en formulario editable
              6. Usuario edita si es necesario y completa opciones de respuesta
              7. Crea la pregunta normalmente
            - âœ… **Resultado Final en el Juego**:
              - Las preguntas creadas desde OCR aparecen en los juegos normalmente
              - No hay diferencia visual entre preguntas creadas manualmente o desde OCR
              - El texto extraÃ­do se guarda como el texto de la pregunta
              - Los jugadores ven y responden la pregunta normalmente durante el juego

            #### **8. DocumentaciÃ³n:**
            - âœ… **Swagger**: Endpoint documentado en \`swagger.yaml\` con ejemplos
            - âœ… **README**: Instrucciones de configuraciÃ³n de Azure en README
            - âœ… **Ejemplos**: Ejemplos de uso del endpoint en documentaciÃ³n

            ---
            ### **Puntos de EstimaciÃ³n:** 8
            ### **Prioridad:** Media
            ### **Fecha Objetivo:** Lunes 17 Noviembre 2025
            ### **TecnologÃ­as:** Azure Computer Vision OCR, Node.js, Express, React
EOFBODY1

          create_and_add_to_project \
            "[BE] US-VC2: OCR - ExtracciÃ³n de Texto de ImÃ¡genes (17 Nov)" \
            "$OCR_BODY_FILE" \
            "Prioridad: Media" \
            "Sprint VisiÃ³n Computacional - Semana 1 (17-19 Nov 2025)"

          # HU 2: Reconocimiento Facial (13 SP, Alta) - Martes 18 Nov (Ya implementado, documentar/mejorar)
          FACE_BODY_FILE=$(mktemp)
          cat > "$FACE_BODY_FILE" << 'EOFBODY2'
**C (Contexto)**: Los usuarios de BrainBlitz necesitan una forma segura y moderna de autenticarse sin depender Ãºnicamente de contraseÃ±as. El sistema actual permite registro e inicio de sesiÃ³n con email/contraseÃ±a, pero se requiere implementar autenticaciÃ³n biomÃ©trica mediante reconocimiento facial.

            **O (Objetivo)**: Permitir que los usuarios se registren e inicien sesiÃ³n usando reconocimiento facial como mÃ©todo de autenticaciÃ³n alternativo o principal, mejorando la seguridad y la experiencia de usuario.

            **N (Necesidad)**: Proporcionar una opciÃ³n de autenticaciÃ³n sin contraseÃ±a que sea rÃ¡pida, segura y accesible desde dispositivos con cÃ¡mara, reduciendo la fricciÃ³n en el proceso de login y mejorando la seguridad mediante biometrÃ­a.

            **E (Entidad)**: Sistema de autenticaciÃ³n facial, base de datos de usuarios, servicio de reconocimiento facial (DeepFace), frontend con acceso a cÃ¡mara, backend con endpoints de registro y login facial.

            **S (Soporte)**: 
            - Microservicio de reconocimiento facial (facial-service) usando DeepFace
            - Endpoints backend: \`POST /api/face/register\` y \`POST /api/face/login\`
            - Frontend: componentes \`FaceRegister.jsx\` y \`FaceLogin.jsx\`
            - Firebase Firestore para almacenar embeddings faciales
            - Azure Container Instances para desplegar el servicio facial

            **S (SuposiciÃ³n)**: 
            - Los usuarios tienen acceso a dispositivos con cÃ¡mara web
            - El navegador soporta acceso a la cÃ¡mara (getUserMedia API)
            - El servicio de reconocimiento facial estÃ¡ desplegado y accesible
            - Los usuarios estÃ¡n dispuestos a registrar su rostro para autenticaciÃ³n

            **A (Criterios de AceptaciÃ³n)**:

            #### **1. Registro Facial:**
            - âœ… **Endpoint Backend**: Existe \`POST /api/face/register\` que acepta:
              - \`image\`: Imagen en Base64 del rostro del usuario
              - \`token\`: Token de autenticaciÃ³n Firebase vÃ¡lido
            - âœ… **ValidaciÃ³n de Imagen**: El sistema valida que la imagen contenga un rostro visible usando DeepFace
            - âœ… **GeneraciÃ³n de Embeddings**: Se generan embeddings faciales usando modelo VGG-Face
            - âœ… **Almacenamiento**: Los embeddings se guardan en Firebase Firestore asociados al \`userId\`
            - âœ… **PrevenciÃ³n de Duplicados**: Un usuario solo puede tener un registro facial activo
            - âœ… **Respuesta Exitosa**: Retorna \`{ success: true, message: 'Cara registrada exitosamente' }\`
            - âœ… **Manejo de Errores**: Retorna errores claros si no se detecta rostro, token invÃ¡lido, o usuario ya registrado

            #### **2. Login Facial:**
            - âœ… **Endpoint Backend**: Existe \`POST /api/face/login\` que acepta:
              - \`image\`: Imagen en Base64 del rostro del usuario
              - \`email\`: Email del usuario para identificar el registro facial
            - âœ… **BÃºsqueda de Usuario**: El sistema busca el usuario por email en Firebase Auth
            - âœ… **VerificaciÃ³n de Registro**: Valida que el usuario tenga un registro facial previo
            - âœ… **ComparaciÃ³n Facial**: Compara la imagen de login con el embedding almacenado usando DeepFace
            - âœ… **Umbral de Confianza**: La verificaciÃ³n requiere un nivel de confianza mÃ­nimo (ej: 0.7)
            - âœ… **GeneraciÃ³n de Token**: Si la verificaciÃ³n es exitosa, genera un token personalizado de Firebase
            - âœ… **Respuesta Exitosa**: Retorna \`{ success: true, verified: true, customToken, userId, confidence }\`
            - âœ… **Manejo de Errores**: Retorna errores claros si no se detecta rostro, usuario no encontrado, o verificaciÃ³n fallida

            #### **3. Frontend - Registro Facial:**
            - âœ… **Componente FaceRegister**: Existe componente React que:
              - Muestra vista previa de la cÃ¡mara en tiempo real
              - Permite capturar foto del rostro
              - Convierte la imagen a Base64
              - EnvÃ­a la imagen al endpoint \`/api/face/register\`
              - Muestra mensajes de Ã©xito/error al usuario
            - âœ… **Acceso a CÃ¡mara**: Solicita permisos de cÃ¡mara usando \`navigator.mediaDevices.getUserMedia()\`
            - âœ… **ValidaciÃ³n Visual**: Muestra indicador visual cuando se detecta un rostro en la cÃ¡mara
            - âœ… **Manejo de Errores**: Muestra mensajes claros si no hay cÃ¡mara, permisos denegados, o registro fallido

            #### **4. Frontend - Login Facial:**
            - âœ… **Componente FaceLogin**: Existe componente React que:
              - Permite ingresar email del usuario
              - Muestra vista previa de la cÃ¡mara
              - Captura foto del rostro
              - EnvÃ­a email e imagen al endpoint \`/api/face/login\`
              - Maneja la respuesta y autentica al usuario con el token recibido
            - âœ… **IntegraciÃ³n con Auth**: DespuÃ©s de login exitoso, actualiza el estado de autenticaciÃ³n en \`AuthContext\`
            - âœ… **RedirecciÃ³n**: Redirige al usuario a la pÃ¡gina principal despuÃ©s de login exitoso

            #### **5. Seguridad:**
            - âœ… **AutenticaciÃ³n Requerida**: El registro facial requiere token de Firebase vÃ¡lido
            - âœ… **ValidaciÃ³n de Token**: El backend verifica el token antes de procesar el registro
            - âœ… **ProtecciÃ³n de Datos**: Los embeddings faciales se almacenan de forma segura en Firestore
            - âœ… **Rate Limiting**: Los endpoints tienen rate limiting para prevenir ataques

            #### **6. Despliegue:**
            - âœ… **Servicio Facial Desplegado**: El microservicio facial estÃ¡ desplegado en Azure Container Instances
            - âœ… **URL Configurada**: La URL del servicio estÃ¡ configurada en \`.env\` como \`DEEPFACE_SERVICE_URL\`
            - âœ… **Health Check**: El servicio responde correctamente a \`GET /health\`

            #### **7. Pruebas:**
            - âœ… **Pruebas Unitarias**: Existen pruebas para los controladores de registro y login facial
            - âœ… **Pruebas de IntegraciÃ³n**: Se prueban los flujos completos de registro y login
            - âœ… **Pruebas Manuales**: Se verifica que el registro y login funcionen en navegadores Chrome, Firefox y Edge

            ---
            ### **Puntos de EstimaciÃ³n:** 13
            ### **Prioridad:** Alta
            ### **Fecha Objetivo:** Martes 18 Noviembre 2025
            ### **TecnologÃ­as:** DeepFace, Azure Container Instances, Firebase Auth, React
EOFBODY2

          create_and_add_to_project \
            "[BE] US-VC1: Reconocimiento Facial para Login y Registro (18 Nov)" \
            "$FACE_BODY_FILE" \
            "Prioridad: Alta" \
            "Sprint VisiÃ³n Computacional - Semana 1 (17-19 Nov 2025)"

          # HU 3: AnÃ¡lisis de ImÃ¡genes (10 SP, Media) - Jueves 20 Nov
          ANALYSIS_BODY_FILE=$(mktemp)
          cat > "$ANALYSIS_BODY_FILE" << 'EOFBODY3'
**C (Contexto)**: Los usuarios y administradores de BrainBlitz necesitan generar preguntas automÃ¡ticamente a partir de imÃ¡genes. Actualmente, las preguntas se crean manualmente o mediante IA basada en texto. Se requiere un sistema que analice imÃ¡genes y genere descripciones, tags y categorÃ­as automÃ¡ticamente para facilitar la creaciÃ³n de preguntas visuales.

            **O (Objetivo)**: Implementar un sistema de anÃ¡lisis inteligente de imÃ¡genes que genere descripciones automÃ¡ticas, tags, categorÃ­as y metadatos de imÃ¡genes, permitiendo crear preguntas de trivia basadas en contenido visual de forma automÃ¡tica.

            **N (Necesidad)**: Automatizar la generaciÃ³n de contenido para preguntas visuales, mejorar la accesibilidad describiendo imÃ¡genes, y permitir bÃºsqueda y categorizaciÃ³n automÃ¡tica de imÃ¡genes por contenido.

            **E (Entidad)**: Servicio de anÃ¡lisis de imÃ¡genes (Azure Computer Vision), endpoint backend para anÃ¡lisis, frontend para subir y visualizar anÃ¡lisis, base de datos para almacenar metadatos de imÃ¡genes, sistema de generaciÃ³n de preguntas basado en anÃ¡lisis.

            **S (Soporte)**: 
            - Azure Computer Vision API con servicio Analyze Image
            - Endpoint backend: \`POST /api/vision/analyze-image\`
            - Frontend: componente para subir imÃ¡genes y mostrar anÃ¡lisis
            - Variable de entorno: \`AZURE_COMPUTER_VISION_KEY\` y \`AZURE_COMPUTER_VISION_ENDPOINT\`
            - Biblioteca: \`@azure/cognitiveservices-computervision\` o llamadas HTTP REST

            **S (SuposiciÃ³n)**: 
            - Azure Computer Vision estÃ¡ configurado
            - Las imÃ¡genes contienen contenido reconocible (objetos, escenas, texto)
            - Los usuarios tienen permisos para subir imÃ¡genes
            - El anÃ¡lisis puede usarse para generar preguntas automÃ¡ticamente

            **A (Criterios de AceptaciÃ³n)**:

            #### **1. ConfiguraciÃ³n de Azure:**
            - âœ… **API Key Configurada**: Variable \`AZURE_COMPUTER_VISION_KEY\` en \`.env\`
            - âœ… **Endpoint Configurado**: Variable \`AZURE_COMPUTER_VISION_ENDPOINT\` en \`.env\`
            - âœ… **Dependencias Instaladas**: \`@azure/cognitiveservices-computervision\` instalado o uso de \`axios\`/\`fetch\`

            #### **2. Endpoint Backend:**
            - âœ… **Ruta**: Existe \`POST /api/vision/analyze-image\` en \`backend-v1/routes/vision.routes.js\`
            - âœ… **Controlador**: MÃ©todo \`analyzeImage\` en \`visionController.js\`
            - âœ… **AutenticaciÃ³n**: Requiere autenticaciÃ³n vÃ¡lida
            - âœ… **ValidaciÃ³n**: Valida imagen en Base64 o archivo, tamaÃ±o mÃ¡ximo 4MB

            #### **3. IntegraciÃ³n con Azure Analyze Image:**
            - âœ… **Llamada a Azure**: POST a \`https://{endpoint}/vision/v3.2/analyze?visualFeatures=Description,Tags,Categories,Objects,Color\`
            - âœ… **Headers**: Incluye \`Ocp-Apim-Subscription-Key\` y \`Content-Type: application/octet-stream\`
            - âœ… **ParÃ¡metros Visuales**: Solicita Description, Tags, Categories, Objects, Color
            - âœ… **Procesamiento**: Procesa respuesta JSON de Azure correctamente

            #### **4. ExtracciÃ³n de Datos:**
            - âœ… **DescripciÃ³n**: Extrae la descripciÃ³n principal y descripciones alternativas de la imagen
            - âœ… **Tags**: Extrae todos los tags con sus niveles de confianza
            - âœ… **CategorÃ­as**: Extrae categorÃ­as detectadas (abstract, people, outdoor, etc.)
            - âœ… **Objetos**: Extrae objetos detectados con sus bounding boxes
            - âœ… **Colores**: Extrae colores dominantes y acento de color
            - âœ… **Metadatos**: Extrae dimensiones, formato de imagen si estÃ¡ disponible

            #### **5. Respuesta del Endpoint:**
            - âœ… **Formato JSON**: Retorna objeto estructurado con descripciÃ³n, tags, categorÃ­as, objetos y colores
            - âœ… **Niveles de Confianza**: Todos los datos incluyen niveles de confianza
            - âœ… **Ordenamiento**: Tags y categorÃ­as ordenados por confianza descendente

            #### **6. Manejo de Errores:**
            - âœ… **Imagen InvÃ¡lida**: Error 400 con mensaje claro
            - âœ… **Sin Contenido**: Mensaje si no se detecta contenido reconocible
            - âœ… **Error de Azure**: Maneja 401, 429, 500 con mensajes apropiados
            - âœ… **Timeout**: Maneja timeouts con reintentos
            - âœ… **Logging**: Registra errores para debugging

            #### **7. Pruebas:**
            - âœ… **Prueba Unitaria**: Prueba funciÃ³n de anÃ¡lisis con imagen de prueba
            - âœ… **Prueba de IntegraciÃ³n**: Prueba endpoint completo
            - âœ… **Prueba de Diferentes Tipos**: Prueba con imÃ¡genes de arte, geografÃ­a, objetos, personas
            - âœ… **Prueba de Errores**: Verifica manejo de errores

            #### **8. IntegraciÃ³n con el Juego - Frontend:**
            - âœ… **Componente AnÃ¡lisis de ImÃ¡genes**: Existe componente \`ImageAnalysisQuestionCreator.jsx\` en \`frontend-v2/src/components/\` que:
              - Permite subir una imagen (drag & drop o botÃ³n de selecciÃ³n)
              - Muestra preview de la imagen subida
              - BotÃ³n \"Analizar Imagen\" que llama al endpoint \`/api/vision/analyze-image\`
              - Muestra spinner de carga mientras procesa
              - Muestra resultados del anÃ¡lisis en secciones:
                - **DescripciÃ³n Principal**: Texto destacado con la descripciÃ³n de la imagen
                - **Tags Detectados**: Chips/badges con los tags mÃ¡s relevantes (mÃ­nimo confianza 0.7)
                - **CategorÃ­as**: Lista de categorÃ­as detectadas
                - **Objetos Detectados**: Lista de objetos con sus niveles de confianza
            - âœ… **GeneraciÃ³n AutomÃ¡tica de Preguntas**:
              - BotÃ³n \"Generar Pregunta desde AnÃ¡lisis\" que:
                1. Usa la descripciÃ³n principal como base para la pregunta
                2. Usa los tags detectados para sugerir categorÃ­a (Arte, GeografÃ­a, Historia, etc.)
                3. Pre-llena el campo de pregunta con formato: \"Â¿QuÃ© se muestra en esta imagen?\" o \"SegÃºn la imagen, Â¿quÃ© es...?\"
                4. Sugiere opciones de respuesta basadas en tags y objetos detectados
              - El usuario puede editar la pregunta generada antes de guardarla
            - âœ… **IntegraciÃ³n con AIQuestionGenerator**: El componente se integra con el generador de preguntas:
              - Aparece como nueva opciÃ³n \"Crear desde AnÃ¡lisis de Imagen\"
              - El anÃ¡lisis se usa para pre-llenar el formulario de pregunta
              - El usuario completa/edita y crea la pregunta normalmente
            - âœ… **Flujo de Usuario Completo**:
              1. Usuario va a \"Crear Juego\" o \"Generar Preguntas\"
              2. Selecciona opciÃ³n \"Crear desde AnÃ¡lisis de Imagen\"
              3. Sube imagen (ej: foto de monumento, obra de arte, paisaje, objeto)
              4. Sistema analiza imagen y muestra resultados
              5. Usuario revisa descripciÃ³n, tags y objetos detectados
              6. Usuario hace clic en \"Generar Pregunta\"
              7. Sistema pre-llena formulario con pregunta sugerida basada en anÃ¡lisis
              8. Usuario edita pregunta y opciones si es necesario
              9. Usuario selecciona respuesta correcta y crea la pregunta
            - âœ… **Resultado Final en el Juego**:
              - Las preguntas creadas desde anÃ¡lisis aparecen en los juegos con la imagen asociada
              - Durante el juego, los jugadores ven:
                - La imagen en la pregunta (si se guardÃ³)
                - El texto de la pregunta generada desde el anÃ¡lisis
                - Las opciones de respuesta
              - Ejemplo de pregunta generada: \"Â¿QuÃ© monumento histÃ³rico se muestra en la imagen?\" con opciones basadas en tags detectados
              - Los jugadores responden normalmente y el sistema valida la respuesta correcta

            #### **9. DocumentaciÃ³n:**
            - âœ… **Swagger**: Endpoint documentado con ejemplos
            - âœ… **Ejemplos de Uso**: Ejemplos de imÃ¡genes y respuestas esperadas
            - âœ… **GuÃ­a de IntegraciÃ³n**: CÃ³mo usar el anÃ¡lisis para generar preguntas

            ---
            ### **Puntos de EstimaciÃ³n:** 10
            ### **Prioridad:** Media
            ### **Fecha Objetivo:** Jueves 20 Noviembre 2025
            ### **TecnologÃ­as:** Azure Computer Vision Analyze Image, Node.js, Express, React
EOFBODY3

          create_and_add_to_project \
            "[BE] US-VC3: AnÃ¡lisis Inteligente de ImÃ¡genes (20 Nov)" \
            "$ANALYSIS_BODY_FILE" \
            "Prioridad: Media" \
            "Sprint VisiÃ³n Computacional - Semana 2 (20-24 Nov 2025)"

          # HU 4: DetecciÃ³n de Objetos (10 SP, Media) - Viernes 21 Nov
          DETECTION_BODY_FILE=$(mktemp)
          cat > "$DETECTION_BODY_FILE" << 'EOFBODY4'
**C (Contexto)**: Los usuarios de BrainBlitz necesitan crear preguntas visuales donde se identifiquen objetos especÃ­ficos en imÃ¡genes. Por ejemplo, "Â¿QuÃ© objeto aparece en esta imagen?" o "Â¿CuÃ¡ntos objetos de tipo X hay en la imagen?". Actualmente, no existe funcionalidad para detectar y localizar objetos en imÃ¡genes.

            **O (Objetivo)**: Implementar un sistema de detecciÃ³n de objetos que identifique y localice objetos especÃ­ficos dentro de imÃ¡genes, permitiendo crear preguntas interactivas basadas en la detecciÃ³n de objetos y mejorar la experiencia de preguntas visuales.

            **N (Necesidad)**: Habilitar la creaciÃ³n de preguntas visuales interactivas, permitir bÃºsqueda de objetos en imÃ¡genes, y mejorar la accesibilidad describiendo quÃ© objetos estÃ¡n presentes en una imagen.

            **E (Entidad)**: Servicio de detecciÃ³n de objetos (Azure Computer Vision), endpoint backend para detecciÃ³n, frontend para visualizar objetos detectados, base de datos para almacenar detecciones, sistema de preguntas basadas en objetos.

            **S (Soporte)**: 
            - Azure Computer Vision API con Object Detection
            - Endpoint backend: \`POST /api/vision/detect-objects\`
            - Frontend: componente para mostrar objetos detectados con bounding boxes
            - Variable de entorno: \`AZURE_COMPUTER_VISION_KEY\` y \`AZURE_COMPUTER_VISION_ENDPOINT\`
            - Biblioteca: \`@azure/cognitiveservices-computervision\` o HTTP REST

            **S (SuposiciÃ³n)**: 
            - Azure Computer Vision soporta detecciÃ³n de objetos
            - Las imÃ¡genes contienen objetos reconocibles
            - Los usuarios necesitan crear preguntas basadas en objetos detectados

            **A (Criterios de AceptaciÃ³n)**:

            #### **1. ConfiguraciÃ³n de Azure:**
            - âœ… **API Key**: Variable \`AZURE_COMPUTER_VISION_KEY\` configurada
            - âœ… **Endpoint**: Variable \`AZURE_COMPUTER_VISION_ENDPOINT\` configurada
            - âœ… **Dependencias**: LibrerÃ­a de Azure instalada o uso de HTTP REST

            #### **2. Endpoint Backend:**
            - âœ… **Ruta**: Existe \`POST /api/vision/detect-objects\` en \`backend-v1/routes/vision.routes.js\`
            - âœ… **Controlador**: MÃ©todo \`detectObjects\` en \`visionController.js\`
            - âœ… **AutenticaciÃ³n**: Requiere autenticaciÃ³n
            - âœ… **ValidaciÃ³n**: Valida imagen Base64 o archivo, tamaÃ±o mÃ¡ximo 4MB

            #### **3. IntegraciÃ³n con Azure Object Detection:**
            - âœ… **Llamada a Azure**: POST a \`https://{endpoint}/vision/v3.2/detect\`
            - âœ… **Headers**: Incluye \`Ocp-Apim-Subscription-Key\` y \`Content-Type: application/octet-stream\`
            - âœ… **Procesamiento**: Procesa respuesta JSON de Azure con objetos detectados

            #### **4. ExtracciÃ³n de Objetos:**
            - âœ… **Lista de Objetos**: Extrae todos los objetos detectados con nombre, confianza, bounding box y Ã¡rea
            - âœ… **Filtrado por Confianza**: Opcionalmente filtra objetos con confianza < 0.5
            - âœ… **Ordenamiento**: Ordena objetos por confianza descendente o por Ã¡rea

            #### **5. Respuesta del Endpoint:**
            - âœ… **Formato JSON**: Retorna objeto con array de objetos detectados, total de objetos y dimensiones de imagen
            - âœ… **Metadatos**: Incluye total de objetos y dimensiones de imagen
            - âœ… **Coordenadas Normalizadas**: Bounding boxes en coordenadas de pÃ­xeles o normalizadas (0-1)

            #### **6. Funcionalidades Adicionales:**
            - âœ… **BÃºsqueda de Objeto EspecÃ­fico**: ParÃ¡metro opcional \`objectName\` para buscar un objeto especÃ­fico
            - âœ… **Conteo de Objetos**: Retorna conteo de cada tipo de objeto detectado
            - âœ… **AgrupaciÃ³n**: Agrupa objetos del mismo tipo

            #### **7. Manejo de Errores:**
            - âœ… **Imagen InvÃ¡lida**: Error 400 con mensaje claro
            - âœ… **Sin Objetos**: Retorna lista vacÃ­a si no se detectan objetos (no error)
            - âœ… **Error de Azure**: Maneja errores de API con mensajes apropiados
            - âœ… **Timeout**: Maneja timeouts
            - âœ… **Logging**: Registra errores

            #### **8. Pruebas:**
            - âœ… **Prueba Unitaria**: Prueba detecciÃ³n con imagen de prueba
            - âœ… **Prueba de IntegraciÃ³n**: Prueba endpoint completo
            - âœ… **Prueba con MÃºltiples Objetos**: Verifica detecciÃ³n de mÃºltiples objetos
            - âœ… **Prueba de PrecisiÃ³n**: Verifica que objetos se detecten correctamente
            - âœ… **Prueba de Errores**: Verifica manejo de errores

            #### **9. IntegraciÃ³n con el Juego - Frontend:**
            - âœ… **Componente DetecciÃ³n de Objetos**: Existe componente \`ObjectDetectionQuestionCreator.jsx\` en \`frontend-v2/src/components/\` que:
              - Permite subir una imagen (drag & drop o botÃ³n de selecciÃ³n)
              - Muestra preview de la imagen subida
              - BotÃ³n \"Detectar Objetos\" que llama al endpoint \`/api/vision/detect-objects\`
              - Muestra spinner de carga mientras procesa
              - Muestra la imagen con bounding boxes dibujados sobre los objetos detectados
              - Muestra lista de objetos detectados con:
                - Nombre del objeto
                - Nivel de confianza (barra de progreso o porcentaje)
                - PosiciÃ³n en la imagen (coordenadas)
            - âœ… **VisualizaciÃ³n Interactiva**:
              - Al hacer hover sobre un objeto en la lista, se resalta su bounding box en la imagen
              - Al hacer clic en un bounding box en la imagen, se resalta en la lista
              - Filtro por confianza mÃ­nima (slider para ajustar umbral)
              - Contador de objetos por tipo (ej: \"3 guitarras\", \"1 persona\", \"2 sillas\")
            - âœ… **GeneraciÃ³n AutomÃ¡tica de Preguntas**:
              - BotÃ³n \"Crear Pregunta de Objetos\" que:
                1. Genera pregunta tipo: \"Â¿QuÃ© objeto aparece en esta imagen?\" o \"Â¿CuÃ¡ntos [objeto] hay en la imagen?\"
                2. Usa los objetos detectados como opciones de respuesta
                3. Marca automÃ¡ticamente el objeto con mayor confianza como respuesta correcta
                4. Pre-llena el formulario con pregunta y opciones
              - OpciÃ³n para crear preguntas de conteo: \"Â¿CuÃ¡ntos [objeto] hay en la imagen?\"
                - Sistema cuenta objetos del mismo tipo
                - Genera opciones numÃ©ricas (0, 1, 2, 3, 4+)
            - âœ… **IntegraciÃ³n con AIQuestionGenerator**: El componente se integra con el generador:
              - Aparece como opciÃ³n \"Crear Pregunta de DetecciÃ³n de Objetos\"
              - La imagen con objetos detectados se guarda asociada a la pregunta
              - El formulario se pre-llena con pregunta y opciones generadas
            - âœ… **Flujo de Usuario Completo**:
              1. Usuario va a \"Crear Juego\" o \"Generar Preguntas\"
              2. Selecciona opciÃ³n \"Crear Pregunta de DetecciÃ³n de Objetos\"
              3. Sube imagen con objetos visibles (ej: foto de instrumentos, animales, objetos cotidianos)
              4. Sistema detecta objetos y muestra resultados visuales
              5. Usuario revisa objetos detectados y ajusta filtro de confianza si es necesario
              6. Usuario selecciona tipo de pregunta:
                 - \"Â¿QuÃ© objeto es este?\" (identificaciÃ³n)
                 - \"Â¿CuÃ¡ntos [objeto] hay?\" (conteo)
              7. Sistema genera pregunta y opciones automÃ¡ticamente
              8. Usuario edita pregunta y opciones si es necesario
              9. Usuario confirma respuesta correcta y crea la pregunta
            - âœ… **Resultado Final en el Juego**:
              - Las preguntas de detecciÃ³n aparecen en los juegos mostrando:
                - La imagen original (sin bounding boxes para no dar pistas)
                - El texto de la pregunta (ej: \"Â¿QuÃ© objeto musical aparece en la imagen?\")
                - Las opciones de respuesta (objetos detectados)
              - Durante el juego, los jugadores:
                1. Ven la imagen de la pregunta
                2. Leen la pregunta sobre quÃ© objeto identificar o contar
                3. Seleccionan su respuesta entre las opciones
                4. El sistema valida si la respuesta es correcta
              - Ejemplo de pregunta en juego:
                - Imagen: Foto de una guitarra, piano y violÃ­n
                - Pregunta: \"Â¿QuÃ© instrumento musical aparece en la imagen?\"
                - Opciones: [\"Guitarra\", \"Piano\", \"ViolÃ­n\", \"BaterÃ­a\"]
                - Respuesta correcta: \"Guitarra\" (objeto con mayor confianza detectado)

            #### **10. DocumentaciÃ³n:**
            - âœ… **Swagger**: Endpoint documentado con ejemplos
            - âœ… **Ejemplos Visuales**: Ejemplos de imÃ¡genes y objetos detectados
            - âœ… **GuÃ­a de Uso**: CÃ³mo usar detecciÃ³n para crear preguntas

            ---
            ### **Puntos de EstimaciÃ³n:** 10
            ### **Prioridad:** Media
            ### **Fecha Objetivo:** Viernes 21 Noviembre 2025
            ### **TecnologÃ­as:** Azure Computer Vision Object Detection, Node.js, Express, React
EOFBODY4

          create_and_add_to_project \
            "[BE] US-VC4: DetecciÃ³n de Objetos en ImÃ¡genes (21 Nov)" \
            "$DETECTION_BODY_FILE" \
            "Prioridad: Media" \
            "Sprint VisiÃ³n Computacional - Semana 2 (20-24 Nov 2025)"

          # Limpiar archivos temporales
          rm -f "$OCR_BODY_FILE" "$FACE_BODY_FILE" "$ANALYSIS_BODY_FILE" "$DETECTION_BODY_FILE"
      
      # PASO 6: Crea y Vincula las Ramas para cada Issue
      - name: Crear Ramas y Vincularlas a las Issues
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }} # Usamos el PAT para consistencia
        run: |
          MAIN_BRANCH_SHA=$(git rev-parse main)
          gh issue list --state open --json number,title | jq -c '.[]' | while read issue; do
            ISSUE_NUMBER=$(echo $issue | jq -r '.number')
            ISSUE_TITLE=$(echo $issue | jq -r '.title')
            BRANCH_TITLE=$(echo "$ISSUE_TITLE" | iconv -t ascii//TRANSLIT | sed -E 's/\[(BE|FE)\] //g' | sed -E 's/[^a-zA-Z0-9]+/-/g' | sed -E 's/^-+\|-+$//g' | tr '[:upper:]' '[:lower:]')
            BRANCH_NAME="hu-${ISSUE_NUMBER}-${BRANCH_TITLE}"

            if git ls-remote --heads origin "$BRANCH_NAME" | grep -q "$BRANCH_NAME"; then
              echo "La rama '$BRANCH_NAME' ya existe."
            else
              echo "Creando rama '$BRANCH_NAME'..."
              gh api repos/${{ github.repository }}/git/refs --method POST -f ref="refs/heads/${BRANCH_NAME}" -f sha="$MAIN_BRANCH_SHA"
              # Comenta en la issue para vincular la rama
              gh issue comment $ISSUE_NUMBER --body "Se ha creado la rama de trabajo para esta HU: \`$BRANCH_NAME\`"
            fi
          done
